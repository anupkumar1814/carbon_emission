{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAABhCAYAAACwNehEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB2UlEQVR4nO3ZMW4TURhG0QlCms5eaBYxW4IlpE5BGQmhLIAKu5rupaFFd2xpMDbn1K/49RVXHvlpjDEmAP7o060PAPjXCSVAEEqAIJQAQSgBglACBKEECEIJEIQSIHze+vDnr5c973hIr+e3W59wl779eL/1CXfn9e37rU+4S1+fv2x65xclQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAcrYYF3XsSzLWNd1y3N+s9vlbHYdu+3raYwxKqbn83k6Ho/T6XSaDofD3+j3Q7Db5Wx2Hbvty6c3QBBKgCCUAGFTKOd5npZlmeZ53vueh2K3y9nsOnbb16Y/cwD+Zz69AYJQAgShBAgfqFVTywk7IEQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.express import scatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "colors_nude = ['#E9EFC0','#B4E197','#83BD75','#4E944F'] #if wandring why greens ?to reinforce the emotion (prediction dollar$)\n",
    "sns.palplot(sns.color_palette(colors_nude))\n",
    "\n",
    "\n",
    "# Set Style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# see this if allowed  \n",
    "from sklearn.model_selection import GridSearchCV , learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Carbon Emission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of our dataset is \",df.shape)\n",
    "print()\n",
    "print()\n",
    "print(\"The column names and unique values in each column of the dataset are :\",)\n",
    "print()\n",
    "\n",
    "print(df.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Vehcile Type column has empty entries we assume that the person does not own any vehicle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vehicle Type']=df['Vehicle Type'].fillna('No Vehicle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to see a basic corelation of the initial data with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "plt.title('Heatmap of Feature Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only shows corelation of numerical datatypes with each other\n",
    "Thus we can see Monthly Distance Driven affects carbon emission a lot along with Waste Bag Weekly Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see numerical description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write some infernce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen the relationship of numerical data with Carbon Emission\n",
    "Now let us see the relationship of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the numerical datatype columns\n",
    "categorical_columns = list(df.columns[df.dtypes == 'object'])\n",
    "# print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove  these 2 columns as they have list which contains what type of things they do\n",
    "#  we will evaulate these in the end\n",
    "categorical_columns.remove('Recycling')\n",
    "categorical_columns.remove('Cooking_With')\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for categorical_column in categorical_columns:\n",
    " region_cost= df.groupby(categorical_column)['CarbonEmission'].mean() \n",
    " fig = plt.figure(figsize=(8,4))\n",
    " sns.barplot(x=region_cost.index, y=region_cost.values, color=colors_nude[-1])\n",
    " plt.title(str(categorical_column)+' Vs Carbon Emission' ,size = 15)\n",
    " plt.ylabel('Carbon Emission')\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots we can say :\n",
    "\n",
    "1 -> as weight of person increase the Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "    \n",
    "2 -> Males generally have more carbon emission then females \n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "3 -> as meat prefernce of person increase the Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "4 -> as the freuency of shower of person increase the Carbon Emission of the person also increases slightly \n",
    "    Thus we should use a ordinal encoding to encode this feature or we may also drop this feature \n",
    "\n",
    "5 -> as  the person use a good source of energy the Carbon Emission of the person also decreases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "6 -> if person used personal vehcile to commute Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "7 -> if person uses a bad source of fuel Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "8 -> Social activity increases carbon emisison of person\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "9 -> as frequecny of air travel of person increase the Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "10 -> as waste bag size of person increase the Carbon Emission of the person also increases\n",
    "    Thus we should use a ordinal encoding to encode this feature \n",
    "\n",
    "\n",
    "11 -> as person used more eneryg effiecnt things Carbon Emission of the person also decreases slightly\n",
    "    Thus we should use a ordinal encoding to encode this feature or drop this feature\n",
    "\n",
    "Thus we can say almost all categorcial data are ordinal and this is slef explanatory as we want the perosn to knpow the carbon emission without use heavy data points and just by simple metrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let us see relationship of the leftover 2 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this contains the things the people recycle\n",
    "list_recycling=['Paper','Metal','Glass','Plastic']\n",
    "# and the columns contain what things the perons recycle from the above 4 \n",
    "\n",
    "# this will contain how many person recycle the product\n",
    "count_recycle = np.zeros(4)\n",
    "\n",
    "# this will contain the total carbon emission of the person with sepcifc thing to recycle \n",
    "sum_carbon=np.zeros(4)\n",
    "for i in range(10000):\n",
    "    for j in range(4):\n",
    "        if list_recycling[j] in df['Recycling'][i]:\n",
    "            count_recycle[j]=count_recycle[j]+1\n",
    "            sum_carbon[j]=sum_carbon[j]+df['CarbonEmission'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Paper','Metal','Glass','Plastic']\n",
    "counts = sum_carbon/count_recycle\n",
    "\n",
    "# Create bar plot\n",
    "plt.bar(categories, counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Materials')\n",
    "plt.ylabel('Carbon Emission')\n",
    "plt.title('Material Vs Emission Mean')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see mean carbon emission is same for all \n",
    "\n",
    "Thus it may not be the best idea to separate make dummies of these entires and make 0 ,1 to represnt whether the person recycle the particular item\n",
    "\n",
    "Instead we add the total number of items person recycle and see how it goes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements(string):\n",
    "    # Remove the brackets and split the string by commas\n",
    "    elements = string.strip(\"[]\").split(\", \")\n",
    "    # Count the number of elements\n",
    "    return len(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we saw that no major difference in seeing for different recycling things we see how many things the [person recycle\n",
    "\n",
    "df['Recycling_count'] = df['Recycling'].apply(count_elements)\n",
    "\n",
    "region_cost= df.groupby('Recycling_count')['CarbonEmission'].mean() \n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=region_cost.index, y=region_cost.values, color=colors_nude[-1])\n",
    "plt.title('Recycling_count'+' Vs Carbon Emission' ,size = 15)\n",
    "plt.ylabel('Carbon Emission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is not same for all \n",
    "\n",
    "it decreases monotonically as the number of products recycled by person increases\n",
    "\n",
    "Thus it is good to use this instead of making dummies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this contains the things  people cook with\n",
    "Cooking_With=['Microwave','Airfryer','Oven','Stove','Grill']\n",
    "# this will contain how many items person use \n",
    "count_Cooking_With = np.zeros(5)\n",
    "# this will contain the total carbon emission of the person with sepcifc thing \n",
    "sum_Cooking_With=np.zeros(5)\n",
    "for i in range(10000):\n",
    "    for j in range(5):\n",
    "        if Cooking_With[j] in df['Cooking_With'][i]:\n",
    "            count_Cooking_With[j]=count_Cooking_With[j]+1\n",
    "            sum_Cooking_With[j]=sum_Cooking_With[j]+df['CarbonEmission'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['Microwave','Airfryer','Oven','Stove','Grill']\n",
    "counts = sum_Cooking_With/count_Cooking_With\n",
    "\n",
    "# Create bar plot\n",
    "plt.bar(categories, counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Cooking_With')\n",
    "plt.ylabel('Carbon Emission')\n",
    "plt.title('Cooking_With Vs Emission Mean')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see mean carbon emission is same for all \n",
    "\n",
    "Thus it may not be the best idea to separate make dummies of these entires and make 0 ,1 to represnt whether the person used the particular item just as we saw above\n",
    "\n",
    "Instead we add the total number of items person uses and see how it goes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# as we saw that no major difference in seeing for different recycling things we see how many things the [person recycle\n",
    "\n",
    "df['Cook_count'] = df['Cooking_With'].apply(count_elements)\n",
    "\n",
    "region_cost= df.groupby('Cook_count')['CarbonEmission'].mean() \n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=region_cost.index, y=region_cost.values, color=colors_nude[-1])\n",
    "plt.title('Cook_count'+' Vs Carbon Emission' ,size = 15)\n",
    "plt.ylabel('Carbon Emission')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here though it is not same for all \n",
    "\n",
    "it increase monotonically as the number of products used for cook increase\n",
    "\n",
    "But difference is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scatter plots not used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us start  with Feature Engineering an make changes that we saw above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already encoded the 2 columns for Recycling and Cook item\n",
    "We just drop the 2 orihinal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Recycling', 'Cooking_With'], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us encode the categorical columns we abopve saw with ordinal encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encode_df(dff, order=None):\n",
    "    mapping = {}\n",
    "    encoded_df = pd.DataFrame()\n",
    "    \n",
    "    # we will use our own order to encode \n",
    "    for col in dff.columns:\n",
    "        if order and col in order:\n",
    "            unique_categories = order[col]\n",
    "        else:\n",
    "            unique_categories = sorted(dff[col].unique())  # Get unique categories and sort them\n",
    "            \n",
    "        category_mapping = {category: idx for idx, category in enumerate(unique_categories)}\n",
    "        mapping[col] = category_mapping\n",
    "        encoded_df[col] = dff[col].map(category_mapping)\n",
    "    \n",
    "    return encoded_df, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    'Body Type': {'overweight': 2, 'obese':3, 'underweight': 0, 'normal': 1},\n",
    "'Sex': {'female': 0, 'male': 1}, \n",
    "'Diet': {'pescatarian': 0, 'vegetarian': 1, 'omnivore': 2, 'vegan': 3},\n",
    "'How Often Shower': {'daily': 1, 'less frequently': 0, 'more frequently': 3, 'twice a day': 2},\n",
    "'Heating Energy Source': {'coal':3, 'natural gas': 1, 'wood': 2, 'electricity': 0},\n",
    "'Transport': {'public': 1, 'walk/bicycle': 0, 'private': 2},\n",
    "'Vehicle Type': { 'no_vehicle': 0, 'petrol': 5, 'diesel': 4, 'hybrid': 3, 'lpg': 2, 'electric': 1},\n",
    "'Social Activity': {'often': 2, 'never': 0, 'sometimes': 1},\n",
    "'Frequency of Traveling by Air': {'frequently': 2, 'rarely': 1, 'never': 0, 'very frequently': 3},\n",
    "'Waste Bag Size': {'large': 2, 'extra large': 3, 'small': 0, 'medium': 1},\n",
    "'Energy efficiency': {'No': 2, 'Sometimes': 1, 'Yes': 0}\n",
    "}\n",
    "\n",
    "df_encoded = df.copy()\n",
    "\n",
    "df_encoded, mapping = ordinal_encode_df(df_encoded, order=orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['Vehicle Type']=df_encoded['Vehicle Type'].fillna(0)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the data in all numerical so let use see the corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = df_encoded.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(corr, cmap='coolwarm')\n",
    "\n",
    "plt.title('Heatmap of Feature Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Vehicle distance depends on Vehicle type and and transport\n",
    "\n",
    "Also We can see more corelation of featiures with Carbon emision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "almost for every feature mean is min + max by 2 thus good data distibution\n",
    "except vehcile type as 0 for many  and air freq, vehcule monthly km and thus carbon emission is not mean , \n",
    "cook count and recycle also not folllow this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_encoded.corr()['CarbonEmission']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as corelation for some feature is very less thus we can remove how often shower ,social activity(diffcilut otjudege) , how long pc , how long intenrt , neergy effieicent, cook count in linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing column whose correlation with 'Carbon emmision' is very low for linear models \n",
    "# df_encoded=df_encoded.drop(columns=['How Long TV PC Daily Hour','How Long Internet Daily Hour', 'How Often Shower', 'Social Activity',  'Cook_count', 'Energy efficiency'])\n",
    "\n",
    "# df_normlaised['total_hrs_on_gadgets']=df_normlaised['How Long TV PC Daily Hour']+df['How Long Internet Daily Hour']\n",
    "# df_normlaised=df_normlaised.drop(columns=['How Long TV PC Daily Hour','How Long Internet Daily Hour'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Manually encode categorical columns\n",
    "def normalise_data(df, columns):\n",
    "   for col in columns:\n",
    "      # Find the maximum value in the column\n",
    "      diff = df[col].max() - df[col].min()\n",
    "      \n",
    "      # Divide each value in the column by its maximum value\n",
    "      df[col] = df[col] / diff\n",
    "   \n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalization\n",
    "columns_to_normlaise = ['Monthly Grocery Bill', 'Vehicle Monthly Distance Km', 'Waste Bag Weekly Count',\n",
    "                     'How Many New Clothes Monthly', 'How Long TV PC Daily Hour','How Long Internet Daily Hour']  # Add your column names here\n",
    "\n",
    "df_normlaised = normalise_data(df_encoded, columns_to_normlaise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving CarbonEmission column at last\n",
    "\n",
    "# Extract the column\n",
    "column_to_move = df_normlaised.pop('CarbonEmission')\n",
    "\n",
    "# Re-insert the column at the last position\n",
    "df_normlaised['CarbonEmission'] = column_to_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normlaised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = df_normlaised.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(corr, cmap='coolwarm')\n",
    "\n",
    "plt.title('Heatmap of Feature Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normlaised.corr()['CarbonEmission']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \n",
    " \n",
    "    # Check if random_state is provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(len(X))\n",
    "    \n",
    "    # Calculate the number of samples in the test set\n",
    "    test_samples = int(len(X) * test_size)\n",
    "    \n",
    "    # Split indices into train and test sets\n",
    "    test_indices = indices[:test_samples]\n",
    "    train_indices = indices[test_samples:]\n",
    "    \n",
    "    # Split the data into train and test sets based on indices\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_bar = np.mean(y_true)\n",
    "    ss_total = np.sum((y_true - y_bar) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r_squared = 1 - (ss_residual / ss_total)\n",
    "    return r_squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree.fit(X[indices], y[indices])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += tree.predict(X)\n",
    "        return predictions / self.n_estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Random Forest without doing feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "numeric_columns = numeric_columns.drop(columns=['Recycling_count', 'Cook_count'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=numeric_columns.iloc[:,0:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=numeric_columns.iloc[:,-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, 0.2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Random Forest model\n",
    "rf_model = xg.XGBoostRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true contains the true target values and y_pred contains the predicted values\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute Root Mean Squared Error (RMSE)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute R-squared (R²)\n",
    "r2_squared = r_squared(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R²):\", r2_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest after feature engineering (converting all categorical column to numerical column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df_encoded.iloc[:,0:18].values,df_encoded.iloc[:,-1].values, 0.2,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Random Forest model\n",
    "rf_model = xg.XGBoostRegressor()\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = rf_model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true contains the true target values and y_pred contains the predicted values\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test1, y_pred1)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test1, y_pred1)\n",
    "\n",
    "# Compute Root Mean Squared Error (RMSE)\n",
    "rmse = root_mean_squared_error(y_test1, y_pred1)\n",
    "\n",
    "# Compute R-squared (R²)\n",
    "r2_squared = r_squared(y_test1, y_pred1)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R²):\", r2_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Normalised Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normlaised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_normlaised.iloc[:,0:18].values,df_normlaised.iloc[:,-1].values, 0.2,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Random Forest model\n",
    "rf_model = xg.XGBoostRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 268.9725907868724\n",
      "Mean Squared Error: 108791.65224106541\n",
      "Root Mean Squared Error: 329.8357958758652\n",
      "R-squared (R²): 0.84164928322474\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_true contains the true target values and y_pred contains the predicted values\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute Root Mean Squared Error (RMSE)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute R-squared (R²)\n",
    "r2_squared = r_squared(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R²):\", r2_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
